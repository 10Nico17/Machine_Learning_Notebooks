{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we implement a very simple version of GPT ourselves.\n",
    "\n",
    "## Grammar Description\n",
    "\n",
    "This grammar generates sentences where a subject (a cat, dog, or bird) performs an action (jumps, runs, or flies) and either goes over another noun or goes through the air.\n",
    "\n",
    "The grammar rules are:\n",
    "\n",
    "- `<start>` ::= `<sentence>`\n",
    "- `<sentence>` ::= `<subject>` `<verb>` `<object>` '.'\n",
    "- `<subject>` ::= 'the' `<noun>`\n",
    "- `<noun>` ::= 'cat' | 'dog' | 'bird'\n",
    "- `<verb>` ::= 'jumps' | 'runs' | 'flies'\n",
    "- `<object>` ::= 'over' 'the' `<noun>` | 'through' 'the' 'air'\n",
    "\n",
    "### Example Sentences\n",
    "\n",
    "- The dog runs over the cat.\n",
    "- The bird flies through the air.\n",
    "- The cat jumps over the bird.\n",
    "- The dog runs through the air.\n",
    "- The bird flies over the dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dog jumps through the air .\n",
      "the bird flies over the dog .\n",
      "the bird flies through the air .\n",
      "the dog runs through the air .\n",
      "the bird flies through the air .\n",
      "the cat runs over the bird .\n",
      "the cat runs through the air .\n",
      "the cat runs over the cat .\n",
      "the dog jumps over the bird .\n",
      "the bird flies through the air .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence():\n",
    "    sentence = generate_subject() + generate_verb() + generate_object() + '.'\n",
    "    return sentence\n",
    "\n",
    "def generate_subject():\n",
    "    return 'the ' + random.choice(['cat', 'dog', 'bird']) + ' '\n",
    "\n",
    "def generate_verb():\n",
    "    return random.choice(['jumps', 'runs', 'flies']) + ' '\n",
    "\n",
    "def generate_object():\n",
    "    if random.random() < 0.5:\n",
    "        return 'over the ' + random.choice(['cat', 'dog', 'bird']) + ' '\n",
    "    else:\n",
    "        return 'through the air '\n",
    "\n",
    "# Generate and save 100 sentences to a text file\n",
    "with open('sentences.txt', 'w') as f:\n",
    "    for i in range(100):\n",
    "        sentence = generate_sentence()\n",
    "        f.write(sentence + '\\n')\n",
    "        if i % 10 == 0:\n",
    "            print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a dataloader loading the created dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, filename, tokenizer, max_len=10):\n",
    "        with open(filename, 'r') as f:\n",
    "            self.sentences = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = '<S> ' + self.sentences[idx] + ' <EOS>'\n",
    "        words = sentence.split()\n",
    "        words = words[:self.max_len] + ['<EOS>'] * max(0, self.max_len - len(words))\n",
    "        return torch.tensor([self.tokenizer[word] for word in words])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the actual GPT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Multi-head attention layers\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norms1 = nn.ModuleList([\n",
    "            torch.nn.LayerNorm(embedding_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Position-wise feedforward layers\n",
    "        self.feedforward_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embedding_dim, embedding_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norms2 = nn.ModuleList([\n",
    "            torch.nn.LayerNorm(embedding_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.unembed = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Embed input sequence\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        # Calculate self-attention for each layer\n",
    "        for attention_layer, feedforward_layer, layer_norm1, layer_norm2 in zip(self.attention_layers, self.feedforward_layers, self.layer_norms1, self.layer_norms2):\n",
    "            pos_enc = torch.zeros(x.shape[1], x.shape[-1])\n",
    "            # calculate the position and dimension values for each element in the matrix\n",
    "            pos = torch.arange(x.shape[1], dtype=torch.float).unsqueeze(1)\n",
    "            div = torch.exp(\n",
    "                torch.arange(0, x.shape[-1], 2).float() *\n",
    "                (-math.log(10000.0) / x.shape[-1])\n",
    "            )\n",
    "            # apply the sin/cos formula to each element in the matrix\n",
    "            pos_enc[:, 0::2] = torch.sin(pos * div)\n",
    "            pos_enc[:, 1::2] = torch.cos(pos * div)\n",
    "            pos_enc = pos_enc.unsqueeze(0)\n",
    "\n",
    "            x = x + pos_enc.to(x)\n",
    "            # Self-attention layer\n",
    "            # TODO create a mask to prevent the model from attending to future tokens and apply the attention layer and save the result in attn_output - 10 points\n",
    "            # Give points if correct triangular shape is present even if inference is not working properly\n",
    "            mask = torch.triu(torch.ones(x.shape[1], x.shape[1]), diagonal=1).bool()\n",
    "            attn_output, _ = attention_layer(x, x, x, attn_mask=mask)\n",
    "            attn_output = attn_output + x\n",
    "            attn_output = layer_norm1(attn_output)\n",
    "\n",
    "            # Position-wise feedforward layer\n",
    "            ff_output = feedforward_layer(attn_output)\n",
    "            ff_output = ff_output + attn_output\n",
    "            ff_output = layer_norm2(ff_output)\n",
    "\n",
    "            # Update input embeddings for next layer\n",
    "            x = ff_output\n",
    "\n",
    "        return self.unembed(x)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement and run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9418391704559326\n",
      "Epoch 2, Loss: 0.7654882431030273\n",
      "Epoch 3, Loss: 0.6414154052734375\n",
      "Epoch 4, Loss: 0.5128526449203491\n",
      "Epoch 5, Loss: 0.4019092321395874\n",
      "Epoch 6, Loss: 0.3272338628768921\n",
      "Epoch 7, Loss: 0.2739838778972626\n",
      "Epoch 8, Loss: 0.24332108497619628\n",
      "Epoch 9, Loss: 0.22117464542388915\n",
      "Epoch 10, Loss: 0.20672646164894104\n",
      "Epoch 11, Loss: 0.2010342001914978\n",
      "Epoch 12, Loss: 0.1858064889907837\n",
      "Epoch 13, Loss: 0.1826074779033661\n",
      "Epoch 14, Loss: 0.18024852275848388\n",
      "Epoch 15, Loss: 0.17430561184883117\n",
      "Epoch 16, Loss: 0.17314277589321136\n",
      "Epoch 17, Loss: 0.17175358831882476\n",
      "Epoch 18, Loss: 0.1722750186920166\n",
      "Epoch 19, Loss: 0.1698059231042862\n",
      "Epoch 20, Loss: 0.1674283802509308\n",
      "Epoch 21, Loss: 0.165815532207489\n",
      "Epoch 22, Loss: 0.16661421656608583\n",
      "Epoch 23, Loss: 0.1698353886604309\n",
      "Epoch 24, Loss: 0.16733944714069365\n",
      "Epoch 25, Loss: 0.16917163729667664\n",
      "Epoch 26, Loss: 0.1636177718639374\n",
      "Epoch 27, Loss: 0.16334621608257294\n",
      "Epoch 28, Loss: 0.1689437747001648\n",
      "Epoch 29, Loss: 0.16316976845264436\n",
      "Epoch 30, Loss: 0.16322460472583772\n",
      "Epoch 31, Loss: 0.16166821420192717\n",
      "Epoch 32, Loss: 0.16194325387477876\n",
      "Epoch 33, Loss: 0.15832698047161103\n",
      "Epoch 34, Loss: 0.15604332387447356\n",
      "Epoch 35, Loss: 0.15724072158336638\n",
      "Epoch 36, Loss: 0.16213878095149994\n",
      "Epoch 37, Loss: 0.15891303718090058\n",
      "Epoch 38, Loss: 0.15700260698795318\n",
      "Epoch 39, Loss: 0.1635221004486084\n",
      "Epoch 40, Loss: 0.16131393015384674\n",
      "Epoch 41, Loss: 0.15795934498310088\n",
      "Epoch 42, Loss: 0.1547845035791397\n",
      "Epoch 43, Loss: 0.15469785332679747\n",
      "Epoch 44, Loss: 0.15451620221138002\n",
      "Epoch 45, Loss: 0.1648923635482788\n",
      "Epoch 46, Loss: 0.15738535523414612\n",
      "Epoch 47, Loss: 0.16124144196510315\n",
      "Epoch 48, Loss: 0.1638844430446625\n",
      "Epoch 49, Loss: 0.15465878546237946\n",
      "Epoch 50, Loss: 0.15921464264392854\n",
      "Epoch 51, Loss: 0.16026457846164704\n",
      "Epoch 52, Loss: 0.1588158428668976\n",
      "Epoch 53, Loss: 0.16372103989124298\n",
      "Epoch 54, Loss: 0.15583117306232452\n",
      "Epoch 55, Loss: 0.15811247527599334\n",
      "Epoch 56, Loss: 0.15729221403598787\n",
      "Epoch 57, Loss: 0.1574041247367859\n",
      "Epoch 58, Loss: 0.1523946166038513\n",
      "Epoch 59, Loss: 0.1573328584432602\n",
      "Epoch 60, Loss: 0.15426011979579926\n",
      "Epoch 61, Loss: 0.1534707576036453\n",
      "Epoch 62, Loss: 0.16305850744247435\n",
      "Epoch 63, Loss: 0.15755383670330048\n",
      "Epoch 64, Loss: 0.15563912093639373\n",
      "Epoch 65, Loss: 0.1542913794517517\n",
      "Epoch 66, Loss: 0.16420270800590514\n",
      "Epoch 67, Loss: 0.15609744787216187\n",
      "Epoch 68, Loss: 0.1582815706729889\n",
      "Epoch 69, Loss: 0.17006345987319946\n",
      "Epoch 70, Loss: 0.15668480098247528\n",
      "Epoch 71, Loss: 0.16110129952430724\n",
      "Epoch 72, Loss: 0.16106514036655425\n",
      "Epoch 73, Loss: 0.15874472558498381\n",
      "Epoch 74, Loss: 0.16003951728343963\n",
      "Epoch 75, Loss: 0.15901345908641815\n",
      "Epoch 76, Loss: 0.15460754334926605\n",
      "Epoch 77, Loss: 0.1592640161514282\n",
      "Epoch 78, Loss: 0.15332773625850676\n",
      "Epoch 79, Loss: 0.15391523838043214\n",
      "Epoch 80, Loss: 0.15431155562400817\n",
      "Epoch 81, Loss: 0.15220873653888703\n",
      "Epoch 82, Loss: 0.15940326750278472\n",
      "Epoch 83, Loss: 0.15297862887382507\n",
      "Epoch 84, Loss: 0.15300135612487792\n",
      "Epoch 85, Loss: 0.15753853023052217\n",
      "Epoch 86, Loss: 0.15955807864665986\n",
      "Epoch 87, Loss: 0.15510694086551666\n",
      "Epoch 88, Loss: 0.15841292440891266\n",
      "Epoch 89, Loss: 0.1555734634399414\n",
      "Epoch 90, Loss: 0.15262983739376068\n",
      "Epoch 91, Loss: 0.1524159938097\n",
      "Epoch 92, Loss: 0.15807355642318727\n",
      "Epoch 93, Loss: 0.15286096036434174\n",
      "Epoch 94, Loss: 0.15117381513118744\n",
      "Epoch 95, Loss: 0.15625721514225005\n",
      "Epoch 96, Loss: 0.15842461287975312\n",
      "Epoch 97, Loss: 0.15190137028694153\n",
      "Epoch 98, Loss: 0.15453848540782927\n",
      "Epoch 99, Loss: 0.150446355342865\n",
      "Epoch 100, Loss: 0.1563837021589279\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define training hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "# Define tokenizer and GPT model\n",
    "tokenizer = {\"<S>\": 0, \"<EOS>\": 1, \"the\": 2, \"dog\": 3, \"cat\": 4, \"bird\": 5, \"flies\": 6, \"jumps\": 7, \"through\": 8, \"air\": 9, \"runs\": 10, \"over\":11, \".\": 12}\n",
    "gpt_model = GPT(vocab_size=len(tokenizer), embedding_dim=64, num_heads=4, num_layers=4)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gpt_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load training data\n",
    "training_data = SentenceDataset(\"sentences.txt\", tokenizer)\n",
    "\n",
    "# Define dataloader\n",
    "dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # TODO create inputd_ids and target_ids based on the current batch for parallel training over time steps - 10 points\n",
    "        input_ids, target_ids = batch[:,:-1], batch[:,1:]\n",
    "        outputs = gpt_model(input_ids)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(outputs.permute(0, 2, 1), target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / 10}\")\n",
    "    running_loss = 0.0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a method for sampling a sentence from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sentence(model, tokenizer, max_len=10):\n",
    "    # TODO implement sentence sampling - 30 points\n",
    "    idx_to_word = {tokenizer[key]: key for key in tokenizer.keys()}\n",
    "\n",
    "    # Start with the \"<S>\" token\n",
    "    input_ids = torch.tensor([tokenizer[\"<S>\"]]).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate tokens until the max length or the \"<EOS>\" token is reached\n",
    "        for i in range(max_len):\n",
    "            # Generate output for the current input\n",
    "            output = model(input_ids)\n",
    "            logits = output[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample the next token\n",
    "            # next_token = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "            # Stop if the \"<EOS>\" token is generated\n",
    "            if next_token == tokenizer[\"<EOS>\"]:\n",
    "                break\n",
    "\n",
    "    # Convert the input_ids tensor to a list of tokens and return as a string\n",
    "    sentence = ' '.join([idx_to_word[idx.item()] for idx in input_ids[0]])\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sample 10 sentences from GPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> the dog runs through the air . <EOS>\n",
      "<S> the bird jumps through the air . <EOS>\n",
      "<S> the bird runs through the air . <EOS>\n",
      "<S> the dog flies through the air . <EOS>\n",
      "<S> the dog runs through the air . <EOS>\n",
      "<S> the cat flies over the dog . <EOS>\n",
      "<S> the bird flies through the air . <EOS>\n",
      "<S> the dog flies over the bird . <EOS>\n",
      "<S> the bird flies over the bird . <EOS>\n",
      "<S> the cat runs over the cat . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sample_sentences(model, tokenizer, num_sentences=10, max_len=10):\n",
    "    # Generate num_sentences sentences\n",
    "    sentences = []\n",
    "    for i in range(num_sentences):\n",
    "        sentence = sample_sentence(model, tokenizer, max_len=max_len)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Generate 10 sample sentences\n",
    "sampled_sentences = sample_sentences(gpt_model, tokenizer, num_sentences=10)\n",
    "\n",
    "for sentence in sampled_sentences:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_peal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
