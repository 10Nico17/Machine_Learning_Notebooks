{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we implement a very simple version of GPT ourselves.\n",
    "\n",
    "## Grammar Description\n",
    "\n",
    "This grammar generates sentences where a subject (a cat, dog, or bird) performs an action (jumps, runs, or flies) and either goes over another noun or goes through the air.\n",
    "\n",
    "The grammar rules are:\n",
    "\n",
    "- `<start>` ::= `<sentence>`\n",
    "- `<sentence>` ::= `<subject>` `<verb>` `<object>` '.'\n",
    "- `<subject>` ::= 'the' `<noun>`\n",
    "- `<noun>` ::= 'cat' | 'dog' | 'bird'\n",
    "- `<verb>` ::= 'jumps' | 'runs' | 'flies'\n",
    "- `<object>` ::= 'over' 'the' `<noun>` | 'through' 'the' 'air'\n",
    "\n",
    "### Example Sentences\n",
    "\n",
    "- The dog runs over the cat.\n",
    "- The bird flies through the air.\n",
    "- The cat jumps over the bird.\n",
    "- The dog runs through the air.\n",
    "- The bird flies over the dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence():\n",
    "    sentence = generate_subject() + generate_verb() + generate_object() + '.'\n",
    "    return sentence\n",
    "\n",
    "def generate_subject():\n",
    "    return 'the ' + random.choice(['cat', 'dog', 'bird']) + ' '\n",
    "\n",
    "def generate_verb():\n",
    "    return random.choice(['jumps', 'runs', 'flies']) + ' '\n",
    "\n",
    "def generate_object():\n",
    "    if random.random() < 0.5:\n",
    "        return 'over the ' + random.choice(['cat', 'dog', 'bird']) + ' '\n",
    "    else:\n",
    "        return 'through the air '\n",
    "\n",
    "# Generate and save 100 sentences to a text file\n",
    "with open('sentences.txt', 'w') as f:\n",
    "    for i in range(100):\n",
    "        sentence = generate_sentence()\n",
    "        f.write(sentence + '\\n')\n",
    "        if i % 10 == 0:\n",
    "            print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a dataloader loading the created dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, filename, tokenizer, max_len=10):\n",
    "        with open(filename, 'r') as f:\n",
    "            self.sentences = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = '<S> ' + self.sentences[idx] + ' <EOS>'\n",
    "        words = sentence.split()\n",
    "        words = words[:self.max_len] + ['<EOS>'] * max(0, self.max_len - len(words))\n",
    "        return torch.tensor([self.tokenizer[word] for word in words])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the actual GPT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Multi-head attention layers\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norms1 = nn.ModuleList([\n",
    "            torch.nn.LayerNorm(embedding_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Position-wise feedforward layers\n",
    "        self.feedforward_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embedding_dim, embedding_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norms2 = nn.ModuleList([\n",
    "            torch.nn.LayerNorm(embedding_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.unembed = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Embed input sequence\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        # Calculate self-attention for each layer\n",
    "        for attention_layer, feedforward_layer, layer_norm1, layer_norm2 in zip(self.attention_layers, self.feedforward_layers, self.layer_norms1, self.layer_norms2):\n",
    "            pos_enc = torch.zeros(x.shape[1], x.shape[-1])\n",
    "            # calculate the position and dimension values for each element in the matrix\n",
    "            pos = torch.arange(x.shape[1], dtype=torch.float).unsqueeze(1)\n",
    "            div = torch.exp(\n",
    "                torch.arange(0, x.shape[-1], 2).float() *\n",
    "                (-math.log(10000.0) / x.shape[-1])\n",
    "            )\n",
    "            # apply the sin/cos formula to each element in the matrix\n",
    "            pos_enc[:, 0::2] = torch.sin(pos * div)\n",
    "            pos_enc[:, 1::2] = torch.cos(pos * div)\n",
    "            pos_enc = pos_enc.unsqueeze(0)\n",
    "\n",
    "            x = x + pos_enc.to(x)\n",
    "            # TODO create a mask to prevent the model from attending to future tokens and apply the attention layer and save the result in attn_output - 10 points\n",
    "            \n",
    "            \n",
    "            attn_output = attn_output + x\n",
    "            attn_output = layer_norm1(attn_output)\n",
    "\n",
    "            # Position-wise feedforward layer\n",
    "            ff_output = feedforward_layer(attn_output)\n",
    "            ff_output = ff_output + attn_output\n",
    "            ff_output = layer_norm2(ff_output)\n",
    "\n",
    "            # Update input embeddings for next layer\n",
    "            x = ff_output\n",
    "\n",
    "        return self.unembed(x)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement and run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define training hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Define tokenizer and GPT model\n",
    "tokenizer = {\"<S>\": 0, \"<EOS>\": 1, \"the\": 2, \"dog\": 3, \"cat\": 4, \"bird\": 5, \"flies\": 6, \"jumps\": 7, \"through\": 8, \"air\": 9, \"runs\": 10, \"over\":11, \".\": 12}\n",
    "gpt_model = GPT(vocab_size=len(tokenizer), embedding_dim=64, num_heads=4, num_layers=4)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gpt_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load training data\n",
    "training_data = SentenceDataset(\"sentences.txt\", tokenizer)\n",
    "\n",
    "# Define dataloader\n",
    "dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # TODO create inputd_ids and target_ids based on the current batch for parallel training over time steps - 10 points\n",
    "\n",
    "        outputs = gpt_model(input_ids)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(outputs.permute(0, 2, 1), target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / 10}\")\n",
    "    running_loss = 0.0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a method for sampling a sentence from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sentence(model, tokenizer, max_len=10):\n",
    "    # TODO implement sentence sampling for the given model that returns the sentence as a string - 30 points\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sample 10 sentences from GPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sentences(model, tokenizer, num_sentences=10, max_len=10):\n",
    "    # Generate num_sentences sentences\n",
    "    sentences = []\n",
    "    for i in range(num_sentences):\n",
    "        sentence = sample_sentence(model, tokenizer, max_len=max_len)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Generate 10 sample sentences\n",
    "sampled_sentences = sample_sentences(gpt_model, tokenizer, num_sentences=10)\n",
    "\n",
    "for sentence in sampled_sentences:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_peal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
