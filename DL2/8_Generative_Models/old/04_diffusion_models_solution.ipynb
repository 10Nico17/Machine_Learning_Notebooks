{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we implement a simple version of generating MNIST with the help of HuggingFace's Diffusers library. We will use the MNIST dataset to train a simple model and then use the trained model to generate new images. We will also generate images from a random noise vector.\n",
    "\n",
    "Hint: You are free to change parts of the script even without a TODO if that helps you. In case your computer is too slow, you can also use Google Colab to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the hugging face diffusers library\n",
    "!pip install diffusers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a training configuration for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# TODO adapt these parameters such that they work for your setup\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 28  # the generated image resolution\n",
    "    num_channels = 1  # the number of channels in the generated image\n",
    "    train_batch_size = 5\n",
    "    eval_batch_size = 2  # how many images to sample during evaluation\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    output_dir = \"samples\"\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "num_train_timesteps=1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the MNIST dataset and the dataloader from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='mnist_data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variance schedule for calculating the alphas and alpha_overline based on https://arxiv.org/pdf/2006.11239.pdf - we use a simple linear schedule here, but quadratic or sinusoidal schedules are also possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(timesteps, num_timesteps, train_timesteps = None):\n",
    "    if not train_timesteps is None:\n",
    "        timesteps = (timesteps * train_timesteps / num_timesteps).to(torch.int32)\n",
    "        num_timesteps = train_timesteps\n",
    "\n",
    "    betas = torch.linspace(0.0001, 0.02, num_timesteps).to(timesteps.device)\n",
    "    return torch.tensor([1 - betas[timesteps[t]] for t in range(len(timesteps))]).to(timesteps.device)\n",
    "\n",
    "def get_alpha_overline(timesteps, num_timesteps, train_timesteps = None):\n",
    "    if not train_timesteps is None:\n",
    "        timesteps = (timesteps * train_timesteps / num_timesteps).to(torch.int32)\n",
    "        num_timesteps = train_timesteps\n",
    "\n",
    "    betas = torch.linspace(0.0001, 0.02, num_timesteps).to(timesteps.device)\n",
    "    return torch.tensor([torch.prod(1 - betas[:timesteps[t]]) for t in range(len(timesteps))]).to(timesteps.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will implement the forward diffusion process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(clean_images, noise, timesteps, num_timesteps):\n",
    "    # TODO implement the forward diffusion process - 10 points\n",
    "    # it takes the clean images, the noise and the timesteps as input and returns the noisy images\n",
    "    alpha = get_alpha_overline(timesteps, num_train_timesteps)[:, None, None, None].to(clean_images.device)\n",
    "    return torch.sqrt(alpha) * clean_images + torch.sqrt(1 - alpha) * noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check forward diffusion process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_show(batch, name, nrow=1):\n",
    "    image_grid = torchvision.utils.make_grid(batch, nrow)\n",
    "    torchvision.utils.save_image(image_grid, name)\n",
    "    Image.open(name)\n",
    "\n",
    "sample_batch = next(iter(train_dataloader))[0]\n",
    "noise = torch.randn_like(sample_batch)\n",
    "save_and_show(sample_batch, 'original.png')\n",
    "for i in range(num_train_timesteps):\n",
    "    if i % 100 == 0:\n",
    "        current_batch = forward_diffusion(sample_batch, noise, i * torch.ones([sample_batch.shape[0]], dtype=torch.int32), num_train_timesteps)\n",
    "        save_and_show(current_batch, f'forward_diffusion_{i}.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we create the actual U-Net model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=1,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=1,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 128),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"UpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the main train loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "def train_loop(config, model, forward_diffusion, optimizer, train_dataloader, device, num_train_timesteps):\n",
    "    model.to(device)\n",
    "    global_step = 0\n",
    "    sample_batch = next(iter(train_dataloader))[0].to(device)\n",
    "    save_and_show(sample_batch, f'reconstruction_original.png')\n",
    "    test_noise = torch.randn_like(sample_batch)\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if epoch * len(train_dataloader) + step in [0, 100, 200, 1000, 5000, 10000, 20000]:\n",
    "                noisy_images_list = []\n",
    "                sample_batch_reconstructed = []\n",
    "                for noise_level in range(11):\n",
    "                    timesteps = int(num_train_timesteps * noise_level / 10) * torch.ones([sample_batch.shape[0]], dtype=torch.int32)\n",
    "                    #timesteps = torch.tensor([0]).to(device)\n",
    "                    # alpha = 1 - (timesteps / num_train_timesteps)[:, None, None, None].to(device)\n",
    "                    alpha = get_alpha_overline(timesteps, num_train_timesteps)[:, None, None, None].to(device)\n",
    "                    # noisy_images = torch.sqrt(alpha) * sample_batch + torch.sqrt(1 - alpha) * noise\n",
    "                    noisy_images = forward_diffusion(sample_batch, test_noise, timesteps, num_train_timesteps)\n",
    "                    noisy_images_list.append(noisy_images)\n",
    "                    noise_pred = model(noisy_images, timesteps.to(device), return_dict=False)[0].detach()\n",
    "                    sample_batch_reconstructed.append((noisy_images - torch.sqrt(1 - alpha) * noise_pred) / torch.sqrt(alpha))\n",
    "                    \n",
    "                save_and_show(torch.cat(noisy_images_list, 0), f'noisy_images_{epoch}_{step}.png', nrow=sample_batch.shape[0])\n",
    "                save_and_show(torch.cat(sample_batch_reconstructed, 0), f'reconstruction_{epoch}_{step}.png', nrow=sample_batch.shape[0])\n",
    "\n",
    "            clean_images = batch[0].to(device)\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, num_train_timesteps, (bs,), device=clean_images.device\n",
    "            ).long().to(device)\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = forward_diffusion(clean_images, noise, timesteps, num_train_timesteps)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(config, model, forward_diffusion, optimizer, train_dataloader, device, num_train_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'unet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('unet_old.pt').to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the utils for evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config, img_name, reverse_diffusion_process, model, timesteps, device, train_timesteps):\n",
    "    #\n",
    "    noise = torch.randn([config.eval_batch_size, config.num_channels, config.image_size, config.image_size])\n",
    "    images = reverse_diffusion_process(model, noise, timesteps, device, train_timesteps)\n",
    "    # Make a grid out of the images\n",
    "    image_grid = torchvision.utils.make_grid(images) #make_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the image grid\n",
    "    torchvision.utils.save_image(image_grid, img_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement the reverse diffusion process with DDPM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion_ddpm(model, noise, num_timesteps, device, train_timesteps):\n",
    "    # TODO implement the reverse diffusion process with DDPM - 15 points\n",
    "    # it should take noise, the model and the number of timesteps as input and return the generated images\n",
    "    # Generate the initial image from the noise\n",
    "    noisy_images_list = []\n",
    "    current_image = noise.to(device)\n",
    "    \n",
    "    # Perform reverse diffusion for the specified number of timesteps\n",
    "    progress_bar = tqdm(range(num_timesteps - 1))\n",
    "    #for t in range(num_timesteps - 1, -1, -1):\n",
    "    for t in range(1, num_timesteps - 1)[::-1]:\n",
    "        # Generate the noise for the current timestep\n",
    "        progress_bar.set_description(f\"T {t}\")\n",
    "        timesteps = torch.ones([current_image.shape[0]], dtype=torch.int32).to(device) * t\n",
    "        current_noise = model(current_image, timesteps, return_dict=False)[0]\n",
    "\n",
    "        #\n",
    "        alpha = get_alpha(timesteps, num_timesteps, train_timesteps)[:,None,None,None]\n",
    "        alpha_overline = get_alpha_overline(timesteps, num_timesteps, train_timesteps)[:,None,None,None]\n",
    "        # sigma = torch.tensor(0).to(device)\n",
    "        sigma = torch.tensor(0.01 * (t / num_timesteps)) if t > 1 else torch.tensor(0).to(device)\n",
    "        z = torch.randn_like(current_image).to(device)\n",
    "\n",
    "        #\n",
    "        current_image = 1 / alpha * (current_image - (1 - alpha) / torch.sqrt(1 - alpha_overline) * current_noise) + sigma * z\n",
    "        current_image = current_image.detach()\n",
    "        if t % (num_timesteps / 10) == 0:\n",
    "            noisy_images_list.append(current_image)\n",
    "    \n",
    "    save_and_show(torch.cat(noisy_images_list, 0), f'ddpm_collage.png', nrow=current_image.shape[0])\n",
    "    # Return the generated images\n",
    "    return current_image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sample from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_inference_timesteps = num_train_timesteps\n",
    "num_inference_timesteps = 200\n",
    "evaluate(config, 'ddpm.png', reverse_diffusion_ddpm, model, num_inference_timesteps, device, num_train_timesteps)\n",
    "Image.open('ddpm.png')\n",
    "\n",
    "# Here the generated MNIST digits should be printed if you implemented it correctly!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid = torchvision.utils.make_grid(next(iter(train_dataloader))[0])\n",
    "torchvision.utils.save_image(image_grid, 'example.png')\n",
    "Image.open('example.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement the reverse diffusion process with DDIM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion_ddim(model, noise, num_timesteps, device, train_timesteps):# TODO implement the reverse diffusion process with DDPM - 15 points\n",
    "    # it should take noise, the model and the number of timesteps as input and return the generated images\n",
    "    # Generate the initial image from the noise\n",
    "    noisy_images_list = []\n",
    "    current_image = noise.to(device)\n",
    "    \n",
    "    # Perform reverse diffusion for the specified number of timesteps\n",
    "    progress_bar = tqdm(range(num_timesteps - 1))\n",
    "    #for t in range(num_timesteps - 1, -1, -1):\n",
    "    for t in range(1, num_timesteps)[::-1]:\n",
    "        # Generate the noise for the current timestep\n",
    "        progress_bar.set_description(f\"T {t}\")\n",
    "        timesteps = torch.ones([current_image.shape[0]], dtype=torch.int32).to(device) * t\n",
    "        current_noise = model(current_image, timesteps, return_dict=False)[0]\n",
    "\n",
    "        #\n",
    "        #alpha_previous = get_alpha(timesteps - 1, num_timesteps, train_timesteps)[:,None,None,None]\n",
    "        #alpha = get_alpha(timesteps, num_timesteps, train_timesteps)[:,None,None,None]\n",
    "        alpha_previous = get_alpha_overline(timesteps - 1, num_timesteps, train_timesteps)[:,None,None,None]\n",
    "        alpha = get_alpha_overline(timesteps, num_timesteps, train_timesteps)[:,None,None,None]\n",
    "        sigma = torch.tensor(0).to(device)\n",
    "        #sigma = torch.tensor(0.01 * (t / num_timesteps)) if t > 1 else torch.tensor(0).to(device)\n",
    "        z = torch.randn_like(current_image).to(device)\n",
    "\n",
    "        #\n",
    "        next_image = torch.sqrt(alpha_previous) * (current_image - torch.sqrt(1 - alpha) * current_noise) / torch.sqrt(alpha)\n",
    "        next_image += torch.sqrt(1 - alpha_previous - torch.square(sigma)) * current_noise\n",
    "        next_image += sigma * z\n",
    "        current_image = next_image\n",
    "        current_image = current_image.detach()\n",
    "        if t % (num_timesteps / 10) == 0:\n",
    "            noisy_images_list.append(current_image)\n",
    "    \n",
    "    save_and_show(torch.cat(noisy_images_list, 0), f'ddim_collage.png', nrow=current_image.shape[0])\n",
    "    # Return the generated images\n",
    "    return current_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_inference_timesteps = num_train_timesteps\n",
    "num_inference_timesteps = 200\n",
    "evaluate(config, 'ddim.png', reverse_diffusion_ddim, model, num_inference_timesteps, device, num_train_timesteps)\n",
    "Image.open('ddim.png')\n",
    "\n",
    "# Here the generated MNIST digits should be printed if you implemented it correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus exercise:\n",
    "# Compare different num_inference_timesteps and different models (DDPM vs DDIM) and discuss the results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_peal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
