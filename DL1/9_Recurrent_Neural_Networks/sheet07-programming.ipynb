{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'>\n",
    "<tr>\n",
    "<td style='background-color:white'>\n",
    "    <p align=\"left\">\n",
    "    Exercises for the course<br>\n",
    "        <b>Deep Learning 1</b><br>\n",
    "    Winter Semester 2022/23\n",
    "    </p>\n",
    "</td>\n",
    "<td style='background-color:white'>\n",
    "    Machine Learning Group<br>\n",
    "    <b>Faculty IV – Electrical Engineering and Computer Science</b><br>\n",
    "    Technische Universität Berlin\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <h1>Exercise Sheet 7 (programming part)</h1>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "In this homework, our goal is to try out recurrent neural network layers in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Visualizing the data\n",
    "\n",
    "\n",
    "Because gradient computation can be error-prone, we often rely on libraries that incorporate automatic differentiation. In this exercise, we make use of the PyTorch library. You are then asked to compute the error of the neural network within that framework, which will then be automatically differentiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import solution07\n",
    "import utils07 as utils\n",
    "\n",
    "# 1. Get the data and parameters\n",
    "data = utils.getdata()\n",
    "\n",
    "# 2. Visualize the time series\n",
    "plt.plot(data.T)\n",
    "\n",
    "input = torch.from_numpy(data[:, :-1])\n",
    "target = torch.from_numpy(data[:, 1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing a LSTM Network (15 P)\n",
    "\n",
    "Implement a two layer LSTM network with pytorch.nn.LSTMCell with 25 hidden neurons.\n",
    "At each prediction step use a linear projection layer to project the hidden representation back to the original data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Network(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()  # implement two LSTMCell layers  # and one linear projection layer\n",
    "\n",
    "\tdef forward(self, input, prediction_steps=0):\n",
    "\t\t# Compute the LSTM's predictions over the input\n",
    "\t\t# Compute 'prediction_steps' steps of predictions into the future\n",
    "\t\t# Return the concatenated outputs\n",
    "\t\tpass\n",
    "\n",
    "lstm = None\n",
    "# lstm = solution07.exercise1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train the LSTM and Visualize It (15P)\n",
    "\n",
    "As the final part you tasked with implementing the necessary steps of training the previously implemented LSTM.\n",
    "It should return the `loss` variable which will be backpropagated through time by the autograd engine of PyTorch.\n",
    "The training setup is already fixed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "lstm.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm.parameters())\n",
    "num_steps = 100\n",
    "for i in range(num_steps):\n",
    "\t#\n",
    "\t# implement a training step\n",
    "\t# zero the grads, pred, compute loss, backpropagate grad\n",
    "\t#\n",
    "\tif i % (num_steps//10) == 0:\n",
    "\t\tprint(f'Training Progress: {int(i/num_steps*100)}%, Loss:', loss.item())\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfuture = 100\n",
    "\tpred = lstm(input, future=future)\n",
    "\ty = pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.title('Solid Lines are Training Data and Dashed Lines are Predictions into the Future', fontsize=30)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "def draw(yi, color):\n",
    "\tplt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth=2.0)\n",
    "\tplt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth=2.0)\n",
    "\n",
    "draw(y[0], 'r')\n",
    "draw(y[1], 'g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
